import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import os
import glob
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# ================= é…ç½®åŒºåŸŸ =================
CONFIG = {
    # ä½ çš„æ•°æ®è·¯å¾„ (ä¿æŒä¸å˜)
    'data_dir': '/data/zm/12_29_InTensity/',

    # æ–‡ä»¶ååŒ¹é…æ¨¡å¼
    'file_pattern': '*_*_clip.csv',

    # ROI è®¾ç½®
    'roi': {
        'row_min': 400, 'row_max': 499,  # ç¬¬0åˆ—
        'col_min': 0, 'col_max': 1280  # ç¬¬1åˆ—
    },

    # è®­ç»ƒè¶…å‚æ•°
    'seq_len': 2048,
    'batch_size': 32,
    'lr': 1e-3,
    'epochs': 100,
    'device': torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    'save_path': '/data/zm/DeltaTNET_model/best_deltat_model.pth'
}


# ================= 1. æ•°æ®é›†å®šä¹‰ (å·²ä¿®å¤ç¼–ç é—®é¢˜) =================
class DeltaTDataset(Dataset):
    def __init__(self, file_list, config, is_train=True):
        self.seq_len = config['seq_len']
        self.roi = config['roi']

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½ {'è®­ç»ƒ' if is_train else 'æµ‹è¯•'} æ•°æ®...")

        # 1. ä¸´æ—¶åˆ—è¡¨æ”¶é›†æ•°æ® (åªå­˜ Numpy æ•°ç»„ï¼Œå‡å°‘å¯¹è±¡å¼€é”€)
        temp_data_list = []
        temp_label_list = []

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        for file_path in tqdm(file_list, desc="Loading Files"):
            try:
                # ... (è§£ææ–‡ä»¶åå’Œè¯»å– CSV çš„ä»£ç ä¿æŒä¸å˜) ...
                # 1. è§£ææ–‡ä»¶å
                basename = os.path.basename(file_path)
                velocity_str = basename.split('_')[0].replace('mm', '')
                label = float(velocity_str)

                # 2. è¯»å– CSV (å…¼å®¹æ€§è¯»å–)
                try:
                    df = pd.read_csv(file_path, header=None, usecols=[0, 1, 2],
                                     names=['row', 'col', 't_in'], encoding='utf-8')
                except:
                    try:
                        df = pd.read_csv(file_path, header=None, usecols=[0, 1, 2],
                                         names=['row', 'col', 't_in'], encoding='gbk')
                    except:
                        continue

                # 3. ROI è¿‡æ»¤
                mask = (df['row'] >= self.roi['row_min']) & (df['row'] <= self.roi['row_max']) & \
                       (df['col'] >= self.roi['col_min']) & (df['col'] <= self.roi['col_max'])
                valid_data = df[mask]

                # 4. æ ¸å¿ƒé€»è¾‘: æŒ‰åƒç´ æ’åº + åˆ†ç»„å·®åˆ†
                # (è¿™é‡Œä½¿ç”¨ä¹‹å‰æä¾›çš„"å…ˆæŒ‰åæ ‡æ’"çš„æ­£ç¡®é€»è¾‘)
                data_val = valid_data.values
                if len(data_val) < 2: continue

                # æ’åº: Time(2), Col(1), Row(0)
                sort_idx = np.lexsort((data_val[:, 2], data_val[:, 1], data_val[:, 0]))
                sorted_data = data_val[sort_idx]

                # å·®åˆ†
                diffs = sorted_data[1:] - sorted_data[:-1]

                # ç­›é€‰åŒåƒç´ äº‹ä»¶ (d_row==0 & d_col==0)
                valid_pixel_mask = (diffs[:, 0] == 0) & (diffs[:, 1] == 0)
                true_isi = diffs[valid_pixel_mask, 2]

                # å‰”é™¤å¼‚å¸¸å€¼
                true_isi = true_isi[true_isi > 0]

                if len(true_isi) < self.seq_len:
                    continue

                # Log å˜æ¢ (å¼ºåˆ¶è½¬ä¸º float32 ä»¥çœå†…å­˜)
                delta_t = np.log1p(true_isi).astype(np.float32)

                # åˆ‡åˆ†
                num_samples = len(delta_t) // self.seq_len
                for i in range(num_samples):
                    segment = delta_t[i * self.seq_len: (i + 1) * self.seq_len]

                    # å­˜å…¥ä¸´æ—¶åˆ—è¡¨
                    temp_data_list.append(segment)
                    temp_label_list.append(label)

            except Exception as e:
                pass  # å¿½ç•¥é”™è¯¯æ–‡ä»¶

        # 2. ğŸŒŸ å…³é”®ä¼˜åŒ–ï¼šå°†åˆ—è¡¨è½¬æ¢ä¸ºç´§å‡‘çš„ Tensor ğŸŒŸ
        # è¿™ä¼šé‡Šæ”¾æ‰åˆ—è¡¨äº§ç”Ÿçš„å·¨å¤§é¢å¤–å¼€é”€
        print("âš¡ï¸ æ­£åœ¨è¿›è¡Œå†…å­˜å‹ç¼© (List -> Tensor)...")
        if len(temp_data_list) > 0:
            # data_tensor æœ¬æ¥å°±æ˜¯ä» numpy è½¬è¿‡æ¥çš„ï¼Œä¿æŒ np.float32 æ²¡é—®é¢˜ï¼ˆtorch.from_numpy ä¼šè‡ªåŠ¨æ¨æ–­ï¼‰
            self.data_tensor = torch.from_numpy(np.array(temp_data_list, dtype=np.float32))

            # label_tensor æ˜¯ç›´æ¥ç”¨ torch.tensor åˆ›å»ºçš„ï¼Œå¿…é¡»ç”¨ torch.float32
            self.label_tensor = torch.tensor(temp_label_list, dtype=torch.float32)  # âœ… ä¿®æ­£ä¸º torch.float32

            # æ ‡ç­¾å½’ä¸€åŒ– (ç›´æ¥åœ¨ Tensor ä¸Šæ“ä½œ)
            # å‡è®¾æœ€å¤§æµé€Ÿ 2.5
            self.label_tensor = self.label_tensor / 2.5
        else:
            self.data_tensor = torch.empty(0)
            self.label_tensor = torch.empty(0)

        # æ‰‹åŠ¨æ¸…ç†ä¸´æ—¶åˆ—è¡¨ï¼Œç«‹åˆ»é‡Šæ”¾å†…å­˜
        del temp_data_list
        del temp_label_list
        import gc
        gc.collect()

        print(f"âœ… åŠ è½½å®Œæˆ: å…± {len(self.data_tensor)} ä¸ªæ ·æœ¬")

    def __len__(self):
        return len(self.data_tensor)

    def __getitem__(self, idx):
        # ç›´æ¥ä» Tensor å–æ•°ï¼Œé€Ÿåº¦æå¿«ä¸”ä¸å é¢å¤–å†…å­˜
        x = self.data_tensor[idx].unsqueeze(0)  # [1, seq_len]
        y = self.label_tensor[idx].unsqueeze(0)  # [1]
        return x, y


# ================= 2. æ¨¡å‹å®šä¹‰ (DeltaTNet) =================
class DeltaTNet(nn.Module):
    def __init__(self):
        super().__init__()

        # 1. Embedding
        self.embedding = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=1),
            nn.BatchNorm1d(64),
            nn.GELU()
        )

        # 2. Dilated TCN
        self.tcn = nn.Sequential(
            nn.Conv1d(64, 64, kernel_size=3, padding=1, dilation=1),
            nn.BatchNorm1d(64), nn.GELU(),
            nn.Conv1d(64, 128, kernel_size=3, padding=2, dilation=2),
            nn.BatchNorm1d(128), nn.GELU(),
            nn.Conv1d(128, 256, kernel_size=3, padding=4, dilation=4),
            nn.BatchNorm1d(256), nn.GELU(),
            nn.Conv1d(256, 256, kernel_size=3, padding=8, dilation=8),
            nn.BatchNorm1d(256), nn.GELU(),
        )

        # 3. Regression
        self.gap = nn.AdaptiveAvgPool1d(1)
        self.regressor = nn.Sequential(
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        x = self.embedding(x)
        x = self.tcn(x)
        x = self.gap(x).flatten(1)
        x = self.regressor(x)
        return x


# ================= 3. è®­ç»ƒæµç¨‹ (å·²æ·»åŠ è¿›åº¦æ¡) =================
def train():
    search_path = os.path.join(CONFIG['data_dir'], CONFIG['file_pattern'])
    all_files = glob.glob(search_path)
    if not all_files:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
        return

    train_files = []
    val_files = []

    print("ğŸ”„ æ­£åœ¨æŒ‰æ–‡ä»¶åè§„åˆ™åˆ‡åˆ†æ•°æ®é›†...")
    for f in all_files:
        basename = os.path.basename(f)
        # æ–‡ä»¶åæ ¼å¼: 0.2_1_clip.csv
        # parts: ['0.2', '1', 'clip.csv']
        try:
            parts = basename.split('_')
            group_idx = parts[1]  # è·å–ä¸­é—´é‚£ä¸ªæ•°å­— '1', '2', '3'

            if group_idx == '3':
                val_files.append(f)  # ç¬¬3ç»„ç”¨äºéªŒè¯
            else:
                train_files.append(f)  # ç¬¬1ã€2ç»„ç”¨äºè®­ç»ƒ
        except IndexError:
            print(f"âš ï¸ æ–‡ä»¶åæ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡: {basename}")
            continue

    print(f"ğŸ“Š æ•°æ®é›†åˆ‡åˆ†ç»“æœ:")
    print(f"   - è®­ç»ƒé›†æ–‡ä»¶æ•°: {len(train_files)} (åŒ…å« _1, _2)")
    print(f"   - éªŒè¯é›†æ–‡ä»¶æ•°: {len(val_files)}   (åŒ…å« _3)")

    # å®‰å…¨æ£€æŸ¥
    if len(train_files) == 0 or len(val_files) == 0:
        print("âŒ åˆ‡åˆ†å¤±è´¥ï¼è¯·æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å« _1, _2, _3 ç»“æ„ã€‚")
        return


    train_ds = DeltaTDataset(train_files, CONFIG, is_train=True)
    val_ds = DeltaTDataset(val_files, CONFIG, is_train=False)

    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)

    model = DeltaTNet().to(CONFIG['device'])
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=1e-4)
    criterion = nn.MSELoss()
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])

    best_loss = float('inf')
    history = {'train_loss': [], 'val_loss': []}

    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")

    for epoch in range(CONFIG['epochs']):
        # --- è®­ç»ƒé˜¶æ®µ ---
        model.train()
        train_loss = 0
        # ğŸŒŸ tqdm è¿›åº¦æ¡: æ˜¾ç¤º Epoch ä¿¡æ¯å’Œå®æ—¶ Loss
        train_bar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{CONFIG['epochs']} [Train]", leave=False)

        for x, y in train_bar:
            x, y = x.to(CONFIG['device']), y.to(CONFIG['device'])

            optimizer.zero_grad()
            pred = model(x)
            loss = criterion(pred, y)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¸Šçš„ Loss æ˜¾ç¤º
            train_bar.set_postfix(loss=f"{loss.item():.4f}")

        train_loss /= len(train_loader)

        # --- éªŒè¯é˜¶æ®µ ---
        model.eval()
        val_loss = 0
        # éªŒè¯é›†é€šå¸¸ä¸éœ€è¦å¤ªè¯¦ç»†çš„è¿›åº¦æ¡ï¼Œç”¨ç®€å•çš„å³å¯
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(CONFIG['device']), y.to(CONFIG['device'])
                pred = model(x)
                val_loss += criterion(pred, y).item()

        val_loss /= len(val_loader)
        scheduler.step()

        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)

        # æ‰“å°æœ¬è½®æ€»ç»“
        print(f"Epoch {epoch + 1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(model.state_dict(), CONFIG['save_path'])
            print(f"   ğŸ’¾ æ¨¡å‹ä¿å­˜ (New Best: {best_loss:.4f})")

    # ç»˜å›¾
    plt.figure(figsize=(10, 6))
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val Loss')
    plt.title('Loss Convergence (DeltaTNet)')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('/data/zm/12.30/training_curve.png')
    print("\nâœ… è®­ç»ƒç»“æŸï¼æ”¶æ•›å›¾å·²ä¿å­˜è‡³ training_curve.png")


if __name__ == "__main__":
    train()