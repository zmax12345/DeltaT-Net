import torch
import numpy as np
import matplotlib.pyplot as plt
import os
import glob
from tqdm import tqdm
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# å¯¼å…¥ä½ çš„æ¨¡å‹å’Œæ•°æ®é›†ç±»
# âš ï¸ æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ å·²ç»æŠŠ dataset.py å’Œ model.py æ”¾åœ¨åŒçº§ç›®å½•
from dataset import DeltaTSequenceDataset
from model import DeltaTNet

# ================= è¯„ä¼°é…ç½® =================
EVAL_CONFIG = {
    # 1. æ•°æ®è·¯å¾„ (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„æ ¼å¼)
    'data_dir': '/data/zm/12_29_InTensity/',
    'file_pattern': '*_*_clip.csv',

    # 2. æ¨¡å‹è·¯å¾„
    'model_dir': '/data/zm/DeltaTNET_model/',
    # å¦‚æœæƒ³æŒ‡å®šç‰¹å®šæ¨¡å‹ï¼Œå¡«æ–‡ä»¶åï¼Œå¦åˆ™å¡« None (è‡ªåŠ¨æ‰¾æœ€æ–°çš„)
    'manual_model_name': None,

    # 3. ç‰©ç†å‚æ•°
    'norm_factor': 2.5,  # è®­ç»ƒæ—¶é™¤ä»¥äº†2.5ï¼Œè¿™é‡Œè¦ä¹˜å›æ¥

    # 4. ç¡¬ä»¶
    'device': torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    'batch_size': 64
}


def load_test_data():
    """
    é‡æ–°å¤ç°è®­ç»ƒæ—¶çš„åˆ‡åˆ†é€»è¾‘ï¼Œåªæå–æµ‹è¯•é›†æ–‡ä»¶
    """
    search_path = os.path.join(EVAL_CONFIG['data_dir'], EVAL_CONFIG['file_pattern'])
    all_files = glob.glob(search_path)

    # å¤ç”¨ train.py ä¸­çš„åˆ‡åˆ†é€»è¾‘ (sklearn random_state=42)
    from sklearn.model_selection import train_test_split
    _, test_files = train_test_split(all_files, test_size=0.2, random_state=42)

    print(f"ğŸ“Š æå–æµ‹è¯•é›†: å…± {len(test_files)} ä¸ªæ–‡ä»¶")

    # æ„é€  Dataset (æ³¨æ„: is_train=False ä¼šè¯»å– files_test)
    # æˆ‘ä»¬è¿™é‡Œæ‰‹åŠ¨æ„é€ ä¸€ä¸ª config å­—å…¸ä¼ ç»™ Dataset
    dummy_config = {
        'files_test': test_files,
        'seq_len': 2048,  # å¿…é¡»ä¸è®­ç»ƒä¸€è‡´
        'roi': {'row_min': 400, 'row_max': 499, 'col_min': 0, 'col_max': 1280}
    }

    ds = DeltaTSequenceDataset(dummy_config, is_train=False)
    loader = torch.utils.data.DataLoader(ds, batch_size=EVAL_CONFIG['batch_size'], shuffle=False)
    return loader


def get_best_model_path():
    if EVAL_CONFIG['manual_model_name']:
        path = os.path.join(EVAL_CONFIG['model_dir'], EVAL_CONFIG['manual_model_name'])
        if os.path.exists(path): return path
        print(f"âŒ æŒ‡å®šæ¨¡å‹ä¸å­˜åœ¨: {path}")

    # è‡ªåŠ¨æ‰¾æœ€æ–°çš„
    files = glob.glob(os.path.join(EVAL_CONFIG['model_dir'], "*.pth"))
    if not files:
        raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .pth æ¨¡å‹æ–‡ä»¶")

    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    latest_file = max(files, key=os.path.getmtime)
    print(f"ğŸ” è‡ªåŠ¨é€‰æ‹©æœ€æ–°æ¨¡å‹: {os.path.basename(latest_file)}")
    return latest_file


def evaluate():
    # 1. å‡†å¤‡æ•°æ®
    test_loader = load_test_data()

    # 2. åŠ è½½æ¨¡å‹
    model_path = get_best_model_path()
    model = DeltaTNet(seq_len=2048).to(EVAL_CONFIG['device'])

    # åŠ è½½æƒé‡ (å¤„ç†å¯èƒ½çš„ DataParallel module å‰ç¼€)
    state_dict = torch.load(model_path, map_location=EVAL_CONFIG['device'])
    new_state_dict = {}
    for k, v in state_dict.items():
        name = k.replace("module.", "")  # å»æ‰å¤šå¡è®­ç»ƒçš„å‰ç¼€
        new_state_dict[name] = v
    model.load_state_dict(new_state_dict)
    model.eval()

    # 3. æ¨ç†
    preds = []
    trues = []

    print("ğŸš€ å¼€å§‹æ¨ç†è¯„ä¼°...")
    with torch.no_grad():
        for x, y in tqdm(test_loader):
            x = x.to(EVAL_CONFIG['device'])

            # å‰å‘ä¼ æ’­
            output = model(x)

            # åå½’ä¸€åŒ– (è¿˜åŸä¸ºçœŸå® mm/s)
            pred_real = output.cpu().numpy().flatten() * EVAL_CONFIG['norm_factor']
            true_real = y.numpy().flatten() * EVAL_CONFIG['norm_factor']

            preds.extend(pred_real)
            trues.extend(true_real)

    preds = np.array(preds)
    trues = np.array(trues)

    # 4. è®¡ç®—æŒ‡æ ‡
    r2 = r2_score(trues, preds)
    rmse = np.sqrt(mean_squared_error(trues, preds))
    mae = mean_absolute_error(trues, preds)

    print("\n" + "=" * 40)
    print(f"ğŸ† æœ€ç»ˆè¯„ä¼°ç»“æœ")
    print(f"RÂ² Score : {r2:.4f}  (è¶Šæ¥è¿‘1è¶Šå¥½)")
    print(f"RMSE     : {rmse:.4f} mm/s")
    print(f"MAE      : {mae:.4f} mm/s")
    print("=" * 40 + "\n")

    # 5. ç»˜å›¾
    plt.figure(figsize=(12, 5))

    # å›¾1: æ•£ç‚¹å›¾ (é¢„æµ‹å€¼ vs çœŸå®å€¼)
    plt.subplot(1, 2, 1)
    plt.scatter(trues, preds, alpha=0.05, s=2, color='blue', label='Samples')
    # ç”»å¯¹è§’çº¿ y=x
    mi = min(trues.min(), preds.min())
    ma = max(trues.max(), preds.max())
    plt.plot([mi, ma], [mi, ma], 'r--', linewidth=2, label='Perfect Fit')
    plt.xlabel('True Velocity (mm/s)')
    plt.ylabel('Predicted Velocity (mm/s)')
    plt.title(f'True vs Predicted (RÂ²={r2:.3f})')
    plt.legend()
    plt.grid(alpha=0.3)

    # å›¾2: è¯¯å·®ç›´æ–¹å›¾
    plt.subplot(1, 2, 2)
    errors = preds - trues
    plt.hist(errors, bins=100, color='purple', alpha=0.7)
    plt.axvline(0, color='r', linestyle='--')
    plt.xlabel('Error (mm/s)')
    plt.ylabel('Count')
    plt.title(f'Error Distribution (RMSE={rmse:.3f})')
    plt.grid(alpha=0.3)

    save_path = 'evaluation_result.png'
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")


if __name__ == "__main__":
    evaluate()